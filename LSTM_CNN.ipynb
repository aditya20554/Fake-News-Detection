{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "EEODXnFvreGX"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os.path\n",
        "import pickle \n",
        "import numpy as np\n",
        "import keras.utils\n",
        "import time\n",
        "from keras.callbacks import TensorBoard, CSVLogger\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten,LSTM,Conv1D,GlobalMaxPool1D,Dropout,Bidirectional\n",
        "# from keras.layers.embedding import Embedding\n",
        "from keras import optimizers\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.models import load_model\n",
        "from nltk.corpus import stopwords\n",
        "import operator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3cCijdFQTdX",
        "outputId": "3bc210df-a632-47bb-a81b-a46c8d9e3362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_KrkewVdxA5M",
        "outputId": "ee757a1a-6683-48bd-8b4e-08b097115f0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydot==1.2.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydot==1.2.3\n",
            "  Downloading pydot-1.2.3.tar.gz (20 kB)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from pydot==1.2.3) (3.0.9)\n",
            "Building wheels for collected packages: pydot\n",
            "  Building wheel for pydot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydot: filename=pydot-1.2.3-py3-none-any.whl size=18941 sha256=82813089c72d7b60c83cbec4c8b9f5007ea04199300a41c652d5fc1ae74f20e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/f3/4f/2614983209399831c4b278ae354b87b35cdc070703c5c8611d\n",
            "Successfully built pydot\n",
            "Installing collected packages: pydot\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 1.3.0\n",
            "    Uninstalling pydot-1.3.0:\n",
            "      Successfully uninstalled pydot-1.3.0\n",
            "Successfully installed pydot-1.2.3\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Q0aRemmeslZL"
      },
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv(\"/content/drive/MyDrive/F3_FineGrained_Fake_News_Detection_train.csv\")\n",
        "test_data=pd.read_csv(\"/content/drive/MyDrive/F3_FineGrained_Fake_News_Detection_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Renaming some columns\n",
        "train_data['party']=train_data['party affiliation']\n",
        "train_data['job']=train_data['speakers job title']\n",
        "train_data['state']=train_data['state info']\n",
        "\n",
        "test_data['party']=test_data['party affiliation']\n",
        "test_data['job']=test_data['speakers job title']\n",
        "test_data['state']=test_data['state info']\n",
        "\n",
        "train_data=train_data.drop('party affiliation',axis=1)\n",
        "train_data=train_data.drop('speakers job title',axis=1)\n",
        "train_data=train_data.drop('state info',axis=1)\n",
        "\n",
        "test_data=test_data.drop('party affiliation',axis=1)\n",
        "test_data=test_data.drop('speakers job title',axis=1)\n",
        "test_data=test_data.drop('state info',axis=1)"
      ],
      "metadata": {
        "id": "Ip_28JNJP8od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z44vhifU0HHi"
      },
      "cell_type": "code",
      "source": [
        "label_enc = {\"pants-fire\" : 0, \"false\" : 1, \"barely-true\" : 2, \"half-true\" : 3, \"mostly-true\" : 4, \"true\" : 5}\n",
        "train_data['output'] = train_data['label'].apply(lambda x: label_enc[x])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "\n",
        "train_data['speaker_id']= label_encoder.fit_transform(train_data['speaker'])\n",
        "train_data['state_id']= label_encoder.fit_transform(train_data['state'])\n",
        "train_data['job_id']= label_encoder.fit_transform(train_data['job'])\n",
        "train_data['party_id']= label_encoder.fit_transform(train_data['party'])\n",
        "train_data['subject_id']= label_encoder.fit_transform(train_data['subject'])\n",
        "\n",
        "test_data['speaker_id']= label_encoder.fit_transform(test_data['speaker'])\n",
        "test_data['state_id']= label_encoder.fit_transform(test_data['state'])\n",
        "test_data['job_id']= label_encoder.fit_transform(test_data['job'])\n",
        "test_data['party_id']= label_encoder.fit_transform(test_data['party'])\n",
        "test_data['subject_id']= label_encoder.fit_transform(test_data['subject'])\n",
        "\n"
      ],
      "metadata": {
        "id": "HREeqpoOfrTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data,val_data = train_test_split(train_data, test_size=0.2, random_state = 0)\n"
      ],
      "metadata": {
        "id": "_yH0-t37P8i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV6KZA5eJx4d",
        "outputId": "62480a1c-7b5a-4219-8450-483a92ba7d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "I3B5FXxuiGMs"
      },
      "cell_type": "code",
      "source": [
        "#encoding  statements\n",
        "def get_vocab_dict(train_data):\n",
        "  vocab_dict = {}\n",
        "  if not os.path.exists('vocabulary.p'):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_data['statement'])\n",
        "    vocab_dict = tokenizer.word_index\n",
        "    pickle.dump(vocab_dict, open( \"vocabulary.p\", \"wb\" ))\n",
        "    \n",
        "  else:\n",
        "    vocab_dict = pickle.load(open(\"vocabulary.p\", \"rb\" ))\n",
        "\n",
        "  return vocab_dict\n",
        "\n",
        "## removing stopwords\n",
        "def preprocessing(statement):\n",
        "  statement = [word for word in statement.split(' ') if word not in stopwords.words('english')]\n",
        "  statement = ' '.join(statement)\n",
        "  text = text_to_word_sequence(statement)\n",
        "  val = [0] * 10\n",
        "  val = [vocab_dict[t] for t in text if t in vocab_dict] \n",
        "  return val\n",
        "\n",
        "\n",
        "vocab_dict = get_vocab_dict(train_data)\n",
        "train_data['word_id'] = train_data['statement'].apply(preprocessing)\n",
        "val_data['word_id'] = val_data['statement'].apply(preprocessing)\n",
        "test_data['word_id'] = test_data['statement'].apply(preprocessing)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download en\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkodkMV5J32q",
        "outputId": "e57c81e7-96cf-4501-886d-d63ce6f52ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 31.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.9.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "h0AutVIndcsZ"
      },
      "cell_type": "code",
      "source": [
        "### pos tagging\n",
        "pos_tags = {'ADJ': 'adjective', 'ADP': 'adposition', 'ADV': 'adverb', \n",
        "            'AUX': 'auxiliary verb', 'CONJ': 'coordinating conjunction', \n",
        "            'DET': 'determiner', 'INTJ': 'interjection', 'NOUN': 'noun', \n",
        "            'NUM': 'numeral', 'PART': 'particle', 'PRON': 'pronoun', \n",
        "            'PROPN': 'proper noun', 'PUNCT': 'punctuation', 'X': 'other', \n",
        "            'SCONJ': 'subord conjunction', 'SYM': 'symbol', 'VERB': 'verb'}\n",
        "\n",
        "pos_dict = {'NOUN' : 0, 'VERB' : 1, 'ADP' : 2, 'PROPN' : 3, 'PUNCT' : 4, \n",
        "            'DET' : 5, 'ADJ' : 6, 'NUM' : 7, 'ADV' : 8, 'PRON' : 9, 'X' : 9, \n",
        "            'PART' : 9, 'SYM' : 9, 'INTJ' : 9 }\n",
        "\n",
        "dep_dict = {'ACL' : 0, 'ACOMP' : 1, 'ADVCL' : 2, 'ADVMOD' : 3, 'AGENT' : 4, \n",
        "            'AMOD' : 5, 'APPOS' : 6, 'ATTR' : 7, 'AUX' : 8, 'AUXPASS' : 9, \n",
        "            'CASE' : 10, 'CC' : 11, 'CCOMP' : 12, 'COMPOUND' : 13, 'CONJ' : 14, \n",
        "            'CSUBJ' : 15, 'CSUBJPASS' : 16, 'DATIVE' : 17, 'DEP' : 18, \n",
        "            'DET' : 19, 'DOBJ' : 20, 'EXPL' : 21, 'INTJ' : 22, 'MARK' : 23, \n",
        "            'META' : 24, 'NEG' : 25, 'NOUNMOD' : 26, 'NPMOD' : 27, 'NSUBJ' : 28, \n",
        "            'NSUBJPASS' : 29, 'NUMMOD' : 30, 'OPRD' : 31, 'PARATAXIS' : 32, \n",
        "            'PCOMP' : 33, 'POBJ' : 34, 'POSS' : 35, 'PRECONJ' : 36, 'PREDET' : 37, \n",
        "            'PREP' : 38, 'PRT' : 39, 'PUNCT' : 40, 'QUANTMOD' : 41, \n",
        "            'RELCL' : 42, 'ROOT' : 43, 'XCOMP' : 44}\n",
        "\n",
        "def get_pos(statement):\n",
        "  doc = nlp(statement)\n",
        "  taglist = []\n",
        "  for token in doc:\n",
        "    taglist.append(pos_dict.get(token.pos_,max(pos_dict.values())))\n",
        "  return taglist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['pos_id'] = train_data['statement'].apply(get_pos)\n",
        "val_data['pos_id'] = val_data['statement'].apply(get_pos)\n",
        "test_data['pos_id'] = test_data['statement'].apply(get_pos)\n"
      ],
      "metadata": {
        "id": "uryPCcyrYwiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3zrlk5FVpDrW"
      },
      "cell_type": "code",
      "source": [
        "## word embeddings\n",
        "embeddings = {}\n",
        "with open(\"/content/drive/MyDrive/glove.6B.100d.txt\") as file_object:\n",
        "  for line in file_object:\n",
        "    embedded_word = line.split()\n",
        "    word = embedded_word[0]\n",
        "    embed = np.array(embedded_word[1:], dtype=\"float32\")\n",
        "    embeddings[word.lower()]= embed\n",
        "\n",
        "emb_dimension = 100\n",
        "\n",
        "num_words = len(vocab_dict) + 1\n",
        "emb_matrix = np.zeros((num_words, emb_dimension))\n",
        "for word, i in vocab_dict.items():\n",
        "    emb_vector = embeddings.get(word)\n",
        "    if emb_vector is not None:\n",
        "        emb_matrix[i] = emb_vector\n",
        "\n",
        "emb_index = None\n",
        "pos_embeddings = np.identity(max(pos_dict.values()), dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "IZkbad3OWVa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wjimn22ztSRe"
      },
      "cell_type": "code",
      "source": [
        "###data preprocessing\n",
        "# hyperparameters setting\n",
        "vocab_length = len(vocab_dict.keys())\n",
        "hidden_size = emb_dimension \n",
        "lstm_size = 100\n",
        "num_steps = 15\n",
        "num_epochs = 30\n",
        "batch_size = 40\n",
        "kernel_sizes = [3,3]\n",
        "filter_size = 128\n",
        "\n",
        "\n",
        "X_train = train_data['word_id']\n",
        "X_val = val_data['word_id']\n",
        "X_test = test_data['word_id']\n",
        "\n",
        "Y_train = train_data['output']\n",
        "Y_train = keras.utils.to_categorical(Y_train, num_classes=6)\n",
        "\n",
        "Y_val = val_data['output']\n",
        "Y_val = keras.utils.to_categorical(Y_val, num_classes=6)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=num_steps, padding='post',truncating='post')\n",
        "X_val = pad_sequences(X_val, maxlen=num_steps, padding='post',truncating='post')\n",
        "X_test = pad_sequences(X_test, maxlen=num_steps, padding='post',truncating='post')\n",
        "\n",
        "X_train_pos = train_data['pos_id']\n",
        "X_val_pos = val_data['pos_id']\n",
        "X_test_pos = test_data['pos_id']\n",
        "\n",
        "X_train_pos = pad_sequences(X_train_pos, maxlen=num_steps, padding='post',truncating='post')\n",
        "X_val_pos = pad_sequences(X_val_pos, maxlen=num_steps, padding='post',truncating='post')\n",
        "X_test_pos = pad_sequences(X_test_pos, maxlen=num_steps, padding='post',truncating='post')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3GvwD8OyhuPP"
      },
      "cell_type": "code",
      "source": [
        "use_pos=False\n",
        "def train(model, name, use_pos=False):\n",
        "  sgd = optimizers.SGD(lr=0.25, clipvalue=0.4, nesterov=True)\n",
        "  adam = optimizers.Adam(lr=0.00075, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "  model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['categorical_accuracy'],)\n",
        "  tb = TensorBoard()\n",
        "  csv_logger = keras.callbacks.CSVLogger('training.log')\n",
        "  filepath= name+\"_weights_best.hdf5\"\n",
        "  checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_categorical_accuracy', \n",
        "                                             verbose=1, save_best_only=True, mode='max')\n",
        "         \n",
        "  if use_pos:\n",
        "    model.fit(\n",
        "      {'main_input': X_train, 'pos_input': X_train_pos},\n",
        "      {'main_output': Y_train}, epochs = num_epochs, batch_size = batch_size,\n",
        "      validation_data = (\n",
        "          {'main_input': X_val, 'pos_input': X_val_pos},\n",
        "          {'main_output': Y_val}\n",
        "      ), \n",
        "      callbacks=[tb,csv_logger,checkpoint])\n",
        "  else:\n",
        "    model.fit(\n",
        "      {'main_input': X_train},\n",
        "      {'main_output': Y_train}, epochs = num_epochs, batch_size = batch_size,\n",
        "      validation_data = (\n",
        "          {'main_input': X_val},\n",
        "          {'main_output': Y_val}\n",
        "      ),\n",
        "       callbacks=[tb,csv_logger,checkpoint]\n",
        "      )\n",
        "    \n",
        "  predict_test(model,name,use_pos)\n",
        "      \n",
        "def predict_test(model, name, use_pos=False):   \n",
        "  preds = []\n",
        "  if use_pos:\n",
        "    preds = model.predict([X_test,X_test_pos], batch_size=batch_size, verbose=1)\n",
        "\n",
        "  else:\n",
        "    preds = model.predict([X_test], batch_size=batch_size, verbose=1)\n",
        "\n",
        "  predictions = np.array([np.argmax(pred) for pred in preds])\n",
        "  print(predictions)\n",
        "  df=pd.DataFrame(predictions)\n",
        "  df['id']=range(len(predictions))\n",
        "  df['label']=predictions\n",
        "  df=df.drop(0,axis=1)\n",
        "  df.to_csv('output.csv',index=False)\n",
        "\n",
        "\n",
        "def predict_train(model, name, use_pos=False):\n",
        "  preds = []\n",
        "  if use_pos:\n",
        "    preds = model.predict([X_train,X_train_pos], batch_size=batch_size, verbose=1)\n",
        "\n",
        "  else:\n",
        "    preds = model.predict([X_train], batch_size=batch_size, verbose=1)  \n",
        "  \n",
        "  predictions = np.array([np.argmax(pred) for pred in preds])\n",
        "  return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n"
      ],
      "metadata": {
        "id": "M8Oft5PmS6hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PqcO5UJawPw8"
      },
      "cell_type": "code",
      "source": [
        "filter_without_pos = []\n",
        "filter_with_pos = []\n",
        "\n",
        "\n",
        "\n",
        "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
        "x_stmt = Embedding(vocab_length+1,emb_dimension,weights=[emb_matrix],input_length=num_steps,trainable=False)(statement_input) \n",
        "\n",
        "# pos embed LSTM\n",
        "pos_input = Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
        "x_pos = Embedding(max(pos_dict.values()), max(pos_dict.values()), weights=[pos_embeddings], input_length=num_steps, trainable=False)(pos_input)\n",
        "\n",
        "for kernel in kernel_sizes:\n",
        "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel)(x_stmt)\n",
        "    x_1 = GlobalMaxPool1D()(x_1)\n",
        "    filter_without_pos.append(x_1)\n",
        "    \n",
        "    x_2 = Conv1D(filters=filter_size,kernel_size=kernel)(x_pos)\n",
        "    x_2 = GlobalMaxPool1D()(x_2)\n",
        "    filter_with_pos.append(x_2)\n",
        "    \n",
        "    \n",
        "conv_in1 = keras.layers.concatenate(filter_without_pos)\n",
        "conv_in1 = Dropout(0.6)(conv_in1)\n",
        "conv_in1 = Dense(128, activation='relu')(conv_in1)\n",
        "\n",
        "conv_in2 = keras.layers.concatenate(filter_with_pos)\n",
        "conv_in2 = Dropout(0.6)(conv_in2)\n",
        "conv_in2 = Dense(128, activation='relu')(conv_in2)\n",
        "\n",
        "x = conv_in1\n",
        "if use_pos:\n",
        "    x = keras.layers.concatenate([conv_in1, conv_in2])\n",
        "else:\n",
        "  x = conv_in1\n",
        "\n",
        "\n",
        "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
        "\n",
        "if use_pos:\n",
        "  model_cnn = Model(inputs=[statement_input, pos_input], outputs=[main_output])\n",
        "else:\n",
        "  model_cnn = Model(inputs=[statement_input], outputs=[main_output])\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_cnn,'cnn',use_pos=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0-bLY2KrBta",
        "outputId": "4bd19054-ca56-46a7-c416-8f8dad7fe2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140/144 [============================>.] - ETA: 0s - loss: 1.8078 - categorical_accuracy: 0.1898\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.21409, saving model to cnn_weights_best.hdf5\n",
            "144/144 [==============================] - 11s 10ms/step - loss: 1.8071 - categorical_accuracy: 0.1901 - val_loss: 1.7535 - val_categorical_accuracy: 0.2141\n",
            "Epoch 2/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.7531 - categorical_accuracy: 0.2162\n",
            "Epoch 2: val_categorical_accuracy improved from 0.21409 to 0.21618, saving model to cnn_weights_best.hdf5\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 1.7529 - categorical_accuracy: 0.2170 - val_loss: 1.7536 - val_categorical_accuracy: 0.2162\n",
            "Epoch 3/30\n",
            "139/144 [===========================>..] - ETA: 0s - loss: 1.7469 - categorical_accuracy: 0.2315\n",
            "Epoch 3: val_categorical_accuracy did not improve from 0.21618\n",
            "144/144 [==============================] - 2s 11ms/step - loss: 1.7467 - categorical_accuracy: 0.2309 - val_loss: 1.7497 - val_categorical_accuracy: 0.2134\n",
            "Epoch 4/30\n",
            "139/144 [===========================>..] - ETA: 0s - loss: 1.7363 - categorical_accuracy: 0.2216\n",
            "Epoch 4: val_categorical_accuracy improved from 0.21618 to 0.22106, saving model to cnn_weights_best.hdf5\n",
            "144/144 [==============================] - 1s 9ms/step - loss: 1.7372 - categorical_accuracy: 0.2229 - val_loss: 1.7466 - val_categorical_accuracy: 0.2211\n",
            "Epoch 5/30\n",
            "144/144 [==============================] - ETA: 0s - loss: 1.7276 - categorical_accuracy: 0.2375\n",
            "Epoch 5: val_categorical_accuracy did not improve from 0.22106\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 1.7276 - categorical_accuracy: 0.2375 - val_loss: 1.7468 - val_categorical_accuracy: 0.2211\n",
            "Epoch 6/30\n",
            "141/144 [============================>.] - ETA: 0s - loss: 1.7092 - categorical_accuracy: 0.2488\n",
            "Epoch 6: val_categorical_accuracy improved from 0.22106 to 0.23780, saving model to cnn_weights_best.hdf5\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 1.7105 - categorical_accuracy: 0.2480 - val_loss: 1.7235 - val_categorical_accuracy: 0.2378\n",
            "Epoch 7/30\n",
            "139/144 [===========================>..] - ETA: 0s - loss: 1.7015 - categorical_accuracy: 0.2507\n",
            "Epoch 7: val_categorical_accuracy did not improve from 0.23780\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.7028 - categorical_accuracy: 0.2508 - val_loss: 1.7304 - val_categorical_accuracy: 0.2113\n",
            "Epoch 8/30\n",
            "141/144 [============================>.] - ETA: 0s - loss: 1.6851 - categorical_accuracy: 0.2672\n",
            "Epoch 8: val_categorical_accuracy did not improve from 0.23780\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6858 - categorical_accuracy: 0.2665 - val_loss: 1.7299 - val_categorical_accuracy: 0.2211\n",
            "Epoch 9/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.6673 - categorical_accuracy: 0.2736\n",
            "Epoch 9: val_categorical_accuracy did not improve from 0.23780\n",
            "144/144 [==============================] - 1s 4ms/step - loss: 1.6663 - categorical_accuracy: 0.2738 - val_loss: 1.7224 - val_categorical_accuracy: 0.2343\n",
            "Epoch 10/30\n",
            "143/144 [============================>.] - ETA: 0s - loss: 1.6474 - categorical_accuracy: 0.2918\n",
            "Epoch 10: val_categorical_accuracy did not improve from 0.23780\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6474 - categorical_accuracy: 0.2918 - val_loss: 1.7480 - val_categorical_accuracy: 0.2378\n",
            "Epoch 11/30\n",
            "139/144 [===========================>..] - ETA: 0s - loss: 1.6296 - categorical_accuracy: 0.3074\n",
            "Epoch 11: val_categorical_accuracy did not improve from 0.23780\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6299 - categorical_accuracy: 0.3061 - val_loss: 1.8986 - val_categorical_accuracy: 0.2092\n",
            "Epoch 12/30\n",
            "137/144 [===========================>..] - ETA: 0s - loss: 1.6132 - categorical_accuracy: 0.3186\n",
            "Epoch 12: val_categorical_accuracy did not improve from 0.23780\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6129 - categorical_accuracy: 0.3188 - val_loss: 1.7412 - val_categorical_accuracy: 0.2294\n",
            "Epoch 13/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.5880 - categorical_accuracy: 0.3384\n",
            "Epoch 13: val_categorical_accuracy improved from 0.23780 to 0.25244, saving model to cnn_weights_best.hdf5\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5875 - categorical_accuracy: 0.3389 - val_loss: 1.7428 - val_categorical_accuracy: 0.2524\n",
            "Epoch 14/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.5583 - categorical_accuracy: 0.3460\n",
            "Epoch 14: val_categorical_accuracy did not improve from 0.25244\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5586 - categorical_accuracy: 0.3455 - val_loss: 1.7595 - val_categorical_accuracy: 0.2225\n",
            "Epoch 15/30\n",
            "136/144 [===========================>..] - ETA: 0s - loss: 1.5562 - categorical_accuracy: 0.3489\n",
            "Epoch 15: val_categorical_accuracy did not improve from 0.25244\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5593 - categorical_accuracy: 0.3467 - val_loss: 1.7276 - val_categorical_accuracy: 0.2490\n",
            "Epoch 16/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.5207 - categorical_accuracy: 0.3711\n",
            "Epoch 16: val_categorical_accuracy did not improve from 0.25244\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5215 - categorical_accuracy: 0.3704 - val_loss: 1.7587 - val_categorical_accuracy: 0.2441\n",
            "Epoch 17/30\n",
            "132/144 [==========================>...] - ETA: 0s - loss: 1.5074 - categorical_accuracy: 0.3799\n",
            "Epoch 17: val_categorical_accuracy did not improve from 0.25244\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5066 - categorical_accuracy: 0.3811 - val_loss: 1.8584 - val_categorical_accuracy: 0.2099\n",
            "Epoch 18/30\n",
            "139/144 [===========================>..] - ETA: 0s - loss: 1.4786 - categorical_accuracy: 0.3890\n",
            "Epoch 18: val_categorical_accuracy did not improve from 0.25244\n",
            "144/144 [==============================] - 1s 4ms/step - loss: 1.4798 - categorical_accuracy: 0.3891 - val_loss: 1.7959 - val_categorical_accuracy: 0.2476\n",
            "Epoch 19/30\n",
            "141/144 [============================>.] - ETA: 0s - loss: 1.4629 - categorical_accuracy: 0.4126\n",
            "Epoch 19: val_categorical_accuracy did not improve from 0.25244\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.4628 - categorical_accuracy: 0.4121 - val_loss: 1.8087 - val_categorical_accuracy: 0.2413\n",
            "Epoch 20/30\n",
            "141/144 [============================>.] - ETA: 0s - loss: 1.4483 - categorical_accuracy: 0.4172\n",
            "Epoch 20: val_categorical_accuracy did not improve from 0.25244\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.4489 - categorical_accuracy: 0.4168 - val_loss: 1.8098 - val_categorical_accuracy: 0.2441\n",
            "Epoch 21/30\n",
            "140/144 [============================>.] - ETA: 0s - loss: 1.4193 - categorical_accuracy: 0.4205\n",
            "Epoch 21: val_categorical_accuracy improved from 0.25244 to 0.26709, saving model to cnn_weights_best.hdf5\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.4196 - categorical_accuracy: 0.4193 - val_loss: 1.7770 - val_categorical_accuracy: 0.2671\n",
            "Epoch 22/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.4007 - categorical_accuracy: 0.4375\n",
            "Epoch 22: val_categorical_accuracy did not improve from 0.26709\n",
            "144/144 [==============================] - 1s 4ms/step - loss: 1.4017 - categorical_accuracy: 0.4374 - val_loss: 1.8945 - val_categorical_accuracy: 0.2308\n",
            "Epoch 23/30\n",
            "139/144 [===========================>..] - ETA: 0s - loss: 1.3836 - categorical_accuracy: 0.4518\n",
            "Epoch 23: val_categorical_accuracy did not improve from 0.26709\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.3808 - categorical_accuracy: 0.4540 - val_loss: 1.8847 - val_categorical_accuracy: 0.2462\n",
            "Epoch 24/30\n",
            "137/144 [===========================>..] - ETA: 0s - loss: 1.3766 - categorical_accuracy: 0.4558\n",
            "Epoch 24: val_categorical_accuracy did not improve from 0.26709\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.3818 - categorical_accuracy: 0.4529 - val_loss: 1.8653 - val_categorical_accuracy: 0.2294\n",
            "Epoch 25/30\n",
            "139/144 [===========================>..] - ETA: 0s - loss: 1.3528 - categorical_accuracy: 0.4696\n",
            "Epoch 25: val_categorical_accuracy did not improve from 0.26709\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.3528 - categorical_accuracy: 0.4695 - val_loss: 1.9078 - val_categorical_accuracy: 0.2169\n",
            "Epoch 26/30\n",
            "137/144 [===========================>..] - ETA: 0s - loss: 1.3363 - categorical_accuracy: 0.4715\n",
            "Epoch 26: val_categorical_accuracy did not improve from 0.26709\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.3379 - categorical_accuracy: 0.4695 - val_loss: 1.8250 - val_categorical_accuracy: 0.2420\n",
            "Epoch 27/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.3168 - categorical_accuracy: 0.4782\n",
            "Epoch 27: val_categorical_accuracy did not improve from 0.26709\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.3184 - categorical_accuracy: 0.4772 - val_loss: 1.8642 - val_categorical_accuracy: 0.2266\n",
            "Epoch 28/30\n",
            "140/144 [============================>.] - ETA: 0s - loss: 1.2787 - categorical_accuracy: 0.5027\n",
            "Epoch 28: val_categorical_accuracy did not improve from 0.26709\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.2854 - categorical_accuracy: 0.5003 - val_loss: 1.8144 - val_categorical_accuracy: 0.2538\n",
            "Epoch 29/30\n",
            "136/144 [===========================>..] - ETA: 0s - loss: 1.2670 - categorical_accuracy: 0.5160\n",
            "Epoch 29: val_categorical_accuracy did not improve from 0.26709\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.2729 - categorical_accuracy: 0.5136 - val_loss: 1.8562 - val_categorical_accuracy: 0.2183\n",
            "Epoch 30/30\n",
            "139/144 [===========================>..] - ETA: 0s - loss: 1.2793 - categorical_accuracy: 0.5009\n",
            "Epoch 30: val_categorical_accuracy did not improve from 0.26709\n",
            "144/144 [==============================] - 1s 4ms/step - loss: 1.2839 - categorical_accuracy: 0.4993 - val_loss: 1.8158 - val_categorical_accuracy: 0.2329\n",
            "77/77 [==============================] - 0s 2ms/step\n",
            "[3 3 2 ... 3 4 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_length+1, hidden_size, input_length=num_steps))\n",
        "model_lstm.add(Bidirectional(LSTM(hidden_size)))\n",
        "model_lstm.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "# statement embed LSTM\n",
        "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
        "x = Embedding(vocab_length+1,emb_dimension,weights=[emb_matrix],input_length=num_steps,trainable=False)(statement_input) \n",
        "lstm_in = LSTM(lstm_size,dropout=0.2)(x)\n",
        "\n",
        "\n",
        "\n",
        "# pos embed LSTM\n",
        "pos_input = Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
        "x2 = Embedding(max(pos_dict.values()), max(pos_dict.values()), weights=[pos_embeddings], input_length=num_steps, trainable=False)(pos_input)\n",
        "lstm_in2 = LSTM(lstm_size, dropout=0.2)(x2)\n",
        "\n",
        "\n",
        "if use_pos :\n",
        "  x = keras.layers.concatenate([lstm_in, lstm_in2])\n",
        "else:\n",
        "  x = lstm_in\n",
        "\n",
        "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
        "\n",
        "if use_pos:\n",
        "  model_lstm = Model(inputs=[statement_input, pos_input], outputs=[main_output])\n",
        "else:\n",
        "  model_lstm = Model(inputs=[statement_input], outputs=[main_output])"
      ],
      "metadata": {
        "id": "UTVCxRzY46VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_lstm,'lstm',use_pos=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0nwzYlt4-kb",
        "outputId": "cd62360b-5009-4663-ac9a-0214093974ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "138/144 [===========================>..] - ETA: 0s - loss: 1.7626 - categorical_accuracy: 0.2156\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.18410, saving model to lstm_weights_best.hdf5\n",
            "144/144 [==============================] - 4s 8ms/step - loss: 1.7615 - categorical_accuracy: 0.2170 - val_loss: 1.7845 - val_categorical_accuracy: 0.1841\n",
            "Epoch 2/30\n",
            "136/144 [===========================>..] - ETA: 0s - loss: 1.7390 - categorical_accuracy: 0.2377\n",
            "Epoch 2: val_categorical_accuracy improved from 0.18410 to 0.22803, saving model to lstm_weights_best.hdf5\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 1.7381 - categorical_accuracy: 0.2391 - val_loss: 1.7261 - val_categorical_accuracy: 0.2280\n",
            "Epoch 3/30\n",
            "143/144 [============================>.] - ETA: 0s - loss: 1.7272 - categorical_accuracy: 0.2476\n",
            "Epoch 3: val_categorical_accuracy improved from 0.22803 to 0.25035, saving model to lstm_weights_best.hdf5\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.7272 - categorical_accuracy: 0.2478 - val_loss: 1.7317 - val_categorical_accuracy: 0.2503\n",
            "Epoch 4/30\n",
            "138/144 [===========================>..] - ETA: 0s - loss: 1.7160 - categorical_accuracy: 0.2520\n",
            "Epoch 4: val_categorical_accuracy improved from 0.25035 to 0.25523, saving model to lstm_weights_best.hdf5\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.7172 - categorical_accuracy: 0.2527 - val_loss: 1.7185 - val_categorical_accuracy: 0.2552\n",
            "Epoch 5/30\n",
            "137/144 [===========================>..] - ETA: 0s - loss: 1.7111 - categorical_accuracy: 0.2591\n",
            "Epoch 5: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.7098 - categorical_accuracy: 0.2602 - val_loss: 1.7357 - val_categorical_accuracy: 0.2427\n",
            "Epoch 6/30\n",
            "136/144 [===========================>..] - ETA: 0s - loss: 1.7037 - categorical_accuracy: 0.2653\n",
            "Epoch 6: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.7035 - categorical_accuracy: 0.2672 - val_loss: 1.7222 - val_categorical_accuracy: 0.2552\n",
            "Epoch 7/30\n",
            "143/144 [============================>.] - ETA: 0s - loss: 1.6916 - categorical_accuracy: 0.2738\n",
            "Epoch 7: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6917 - categorical_accuracy: 0.2735 - val_loss: 1.7454 - val_categorical_accuracy: 0.2273\n",
            "Epoch 8/30\n",
            "135/144 [===========================>..] - ETA: 0s - loss: 1.6864 - categorical_accuracy: 0.2787\n",
            "Epoch 8: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6867 - categorical_accuracy: 0.2782 - val_loss: 1.8125 - val_categorical_accuracy: 0.1946\n",
            "Epoch 9/30\n",
            "141/144 [============================>.] - ETA: 0s - loss: 1.6839 - categorical_accuracy: 0.2773\n",
            "Epoch 9: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6841 - categorical_accuracy: 0.2766 - val_loss: 1.7431 - val_categorical_accuracy: 0.2406\n",
            "Epoch 10/30\n",
            "140/144 [============================>.] - ETA: 0s - loss: 1.6773 - categorical_accuracy: 0.2784\n",
            "Epoch 10: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6769 - categorical_accuracy: 0.2778 - val_loss: 1.7666 - val_categorical_accuracy: 0.2287\n",
            "Epoch 11/30\n",
            "141/144 [============================>.] - ETA: 0s - loss: 1.6676 - categorical_accuracy: 0.2878\n",
            "Epoch 11: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6680 - categorical_accuracy: 0.2883 - val_loss: 1.7472 - val_categorical_accuracy: 0.2343\n",
            "Epoch 12/30\n",
            "144/144 [==============================] - ETA: 0s - loss: 1.6581 - categorical_accuracy: 0.2946\n",
            "Epoch 12: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6581 - categorical_accuracy: 0.2946 - val_loss: 1.7780 - val_categorical_accuracy: 0.2078\n",
            "Epoch 13/30\n",
            "141/144 [============================>.] - ETA: 0s - loss: 1.6526 - categorical_accuracy: 0.2979\n",
            "Epoch 13: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6528 - categorical_accuracy: 0.2975 - val_loss: 1.8145 - val_categorical_accuracy: 0.1987\n",
            "Epoch 14/30\n",
            "140/144 [============================>.] - ETA: 0s - loss: 1.6453 - categorical_accuracy: 0.3054\n",
            "Epoch 14: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6451 - categorical_accuracy: 0.3069 - val_loss: 1.7638 - val_categorical_accuracy: 0.2434\n",
            "Epoch 15/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.6368 - categorical_accuracy: 0.3102\n",
            "Epoch 15: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6366 - categorical_accuracy: 0.3111 - val_loss: 1.7656 - val_categorical_accuracy: 0.2413\n",
            "Epoch 16/30\n",
            "141/144 [============================>.] - ETA: 0s - loss: 1.6256 - categorical_accuracy: 0.3167\n",
            "Epoch 16: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6263 - categorical_accuracy: 0.3153 - val_loss: 1.8130 - val_categorical_accuracy: 0.2294\n",
            "Epoch 17/30\n",
            "138/144 [===========================>..] - ETA: 0s - loss: 1.6126 - categorical_accuracy: 0.3254\n",
            "Epoch 17: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6142 - categorical_accuracy: 0.3239 - val_loss: 1.7542 - val_categorical_accuracy: 0.2211\n",
            "Epoch 18/30\n",
            "138/144 [===========================>..] - ETA: 0s - loss: 1.6037 - categorical_accuracy: 0.3257\n",
            "Epoch 18: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.6036 - categorical_accuracy: 0.3268 - val_loss: 1.8143 - val_categorical_accuracy: 0.2134\n",
            "Epoch 19/30\n",
            "133/144 [==========================>...] - ETA: 0s - loss: 1.5898 - categorical_accuracy: 0.3289\n",
            "Epoch 19: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5926 - categorical_accuracy: 0.3291 - val_loss: 1.7916 - val_categorical_accuracy: 0.2329\n",
            "Epoch 20/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.5875 - categorical_accuracy: 0.3386\n",
            "Epoch 20: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5872 - categorical_accuracy: 0.3389 - val_loss: 1.8038 - val_categorical_accuracy: 0.2197\n",
            "Epoch 21/30\n",
            "138/144 [===========================>..] - ETA: 0s - loss: 1.5669 - categorical_accuracy: 0.3500\n",
            "Epoch 21: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5684 - categorical_accuracy: 0.3483 - val_loss: 1.7595 - val_categorical_accuracy: 0.2280\n",
            "Epoch 22/30\n",
            "139/144 [===========================>..] - ETA: 0s - loss: 1.5442 - categorical_accuracy: 0.3696\n",
            "Epoch 22: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5444 - categorical_accuracy: 0.3696 - val_loss: 1.8724 - val_categorical_accuracy: 0.2197\n",
            "Epoch 23/30\n",
            "138/144 [===========================>..] - ETA: 0s - loss: 1.5299 - categorical_accuracy: 0.3757\n",
            "Epoch 23: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5342 - categorical_accuracy: 0.3732 - val_loss: 1.8043 - val_categorical_accuracy: 0.2169\n",
            "Epoch 24/30\n",
            "140/144 [============================>.] - ETA: 0s - loss: 1.5193 - categorical_accuracy: 0.3823\n",
            "Epoch 24: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.5176 - categorical_accuracy: 0.3828 - val_loss: 1.8373 - val_categorical_accuracy: 0.2434\n",
            "Epoch 25/30\n",
            "138/144 [===========================>..] - ETA: 0s - loss: 1.4970 - categorical_accuracy: 0.3909\n",
            "Epoch 25: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.4991 - categorical_accuracy: 0.3894 - val_loss: 1.9121 - val_categorical_accuracy: 0.2127\n",
            "Epoch 26/30\n",
            "140/144 [============================>.] - ETA: 0s - loss: 1.4891 - categorical_accuracy: 0.4004\n",
            "Epoch 26: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.4915 - categorical_accuracy: 0.3987 - val_loss: 1.9544 - val_categorical_accuracy: 0.2050\n",
            "Epoch 27/30\n",
            "140/144 [============================>.] - ETA: 0s - loss: 1.4672 - categorical_accuracy: 0.4061\n",
            "Epoch 27: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.4659 - categorical_accuracy: 0.4063 - val_loss: 1.8757 - val_categorical_accuracy: 0.2343\n",
            "Epoch 28/30\n",
            "135/144 [===========================>..] - ETA: 0s - loss: 1.4398 - categorical_accuracy: 0.4146\n",
            "Epoch 28: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.4400 - categorical_accuracy: 0.4151 - val_loss: 1.8536 - val_categorical_accuracy: 0.2204\n",
            "Epoch 29/30\n",
            "141/144 [============================>.] - ETA: 0s - loss: 1.4229 - categorical_accuracy: 0.4282\n",
            "Epoch 29: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.4202 - categorical_accuracy: 0.4299 - val_loss: 1.9615 - val_categorical_accuracy: 0.2245\n",
            "Epoch 30/30\n",
            "142/144 [============================>.] - ETA: 0s - loss: 1.4042 - categorical_accuracy: 0.4361\n",
            "Epoch 30: val_categorical_accuracy did not improve from 0.25523\n",
            "144/144 [==============================] - 1s 5ms/step - loss: 1.4029 - categorical_accuracy: 0.4363 - val_loss: 1.9533 - val_categorical_accuracy: 0.2280\n",
            "77/77 [==============================] - 0s 2ms/step\n",
            "[5 5 1 ... 4 5 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred= predict_train(model_lstm,'lstm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5Z8J2Sf9wQS",
        "outputId": "bbc76318-25ff-4a2e-a034-231415c61a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144/144 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(train_data['output'], y_pred, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JpNi3B094Jm",
        "outputId": "5c409ac9-2254-43bc-f343-4661f85a6e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5086813276100964"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}